# FZAI Fuzzer - Behind AI Lines: Disrupting LLM Alignment to Build Bombs, Leading to Enhanced Security

## Description
Who would have thought that prompting LLMs with questions about building bombs could actually strengthen their security? As these models become foundational to our digital tools—much like a new operating system—they still lack many essential security features. That's where our approach steps in: by understanding and disrupting their core alignments.

Leveraging our extensive experience in vulnerability research, we apply zero-day discovery techniques to generative AI. We've developed a systematic method to breach the defenses of the latest LLM models, accompanied by a new open-source fuzzing infrastructure that makes jailbreaking not only efficient but also integral to crafting detection-based solutions that enhance LLM security.

Our research goes deeper, examining how manipulating different neuron layers affects alignment. By dissecting these layers, we uncover the mechanics of LLM behavior and find ways to adjust their alignments for greater security.

## Code
https://github.com/cyberark/FuzzyAI
